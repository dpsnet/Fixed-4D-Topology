"""
è·¨ç»´åº¦éªŒè¯
Cross-Dimension Validation

éªŒè¯æ ¸å¿ƒçŒœæƒ³: c1 = 1/d_f^max åœ¨ä¸åŒç»´åº¦æ˜¯å¦æˆç«‹

æµ‹è¯•:
- 3ç»´ç©ºé—´: c1 = 1/3 â‰ˆ 0.333
- 4ç»´ç©ºé—´: c1 = 1/4 = 0.250  
- 5ç»´ç©ºé—´: c1 = 1/5 = 0.200
- 6ç»´ç©ºé—´: c1 = 1/6 â‰ˆ 0.167

å¦‚æœçŒœæƒ³æ­£ç¡®ï¼Œæå–çš„c1åº”è¯¥ä¸1/dimensionæˆæ­£æ¯”
"""

import numpy as np
from optimized_fractal_generator import OptimizedFractalGenerator
from fractal_laplacian import FractalLaplacian
from robust_c1_extractor import RobustC1Extractor
import json
from datetime import datetime


class CrossDimensionValidator:
    """
    è·¨ç»´åº¦éªŒè¯å™¨
    
    éªŒè¯ c1 = 1/d_f^max åœ¨ä¸åŒç»´åº¦æ˜¯å¦æˆç«‹
    """
    
    def __init__(self):
        self.results = {}
        
    def validate_dimension(self, dimension: int, n_samples: int = 10,
                          n_points: int = 600) -> dict:
        """
        éªŒè¯ç‰¹å®šç»´åº¦çš„c1
        
        Args:
            dimension: ç©ºé—´ç»´åº¦ (3, 4, 5, 6)
            n_samples: æ ·æœ¬æ•°
            n_points: æ¯æ ·æœ¬ç‚¹æ•°
            
        Returns:
            éªŒè¯ç»“æœ
        """
        print(f"\n{'='*70}")
        print(f"éªŒè¯ {dimension} ç»´ç©ºé—´")
        print(f"ç†è®º c1 = 1/{dimension} = {1.0/dimension:.6f}")
        print(f"{'='*70}")
        
        # æ ¹æ®ç»´åº¦è®¾ç½®å‚æ•°
        d_min = 2.0  # æœ€å°ç»´åº¦å›ºå®šä¸º2
        d_max = float(dimension)
        c1_theory = 1.0 / dimension
        
        # åˆ›å»ºç”Ÿæˆå™¨å’Œæå–å™¨
        ofg = OptimizedFractalGenerator(dimension=dimension, c1=c1_theory)
        extractor = RobustC1Extractor(true_c1=c1_theory)
        
        c1_values = []
        d_s_ranges = []
        
        for i in range(n_samples):
            if i % 3 == 0:
                print(f"  è¿›åº¦: {i}/{n_samples}")
            
            np.random.seed(i * 100 + dimension)
            
            # ç”Ÿæˆåˆ†å½¢
            try:
                points = ofg.generate_dimension_cascade(
                    n_points=n_points,
                    ell_min=0.001,
                    ell_max=100.0,
                    ell_0=1.0
                )
                
                # è®¡ç®—è°±ç»´åº¦
                fl = FractalLaplacian(dimension=dimension)
                L = fl.construct_graph_laplacian(points, epsilon=None)
                
                t_vals, d_s = fl.compute_spectral_dimension(
                    L,
                    t_range=np.logspace(-1.5, 1.5, 50),
                    n_eigenvalues=min(40, len(points)//5)
                )
                
                ell_vals = np.sqrt(t_vals)
                d_s_range = d_s.max() - d_s.min()
                
                # æå–c1
                result = extractor.extract_weighted_fit(ell_vals, d_s)
                
                if 'c1' in result and 0 < result['c1'] < 2:
                    c1_values.append(result['c1'])
                    d_s_ranges.append(d_s_range)
                    print(f"    æ ·æœ¬{i+1}: c1 = {result['c1']:.4f}, "
                          f"d_sèŒƒå›´ = [{d_s.min():.2f}, {d_s.max():.2f}]")
                
            except Exception as e:
                print(f"    æ ·æœ¬{i+1}: å¤±è´¥ - {str(e)[:50]}")
                continue
        
        # ç»Ÿè®¡åˆ†æ
        if len(c1_values) == 0:
            return {'error': 'No valid c1 extracted', 'dimension': dimension}
        
        c1_values = np.array(c1_values)
        
        summary = {
            'dimension': dimension,
            'n_samples': n_samples,
            'successful': len(c1_values),
            'c1_theory': c1_theory,
            'c1_mean': float(np.mean(c1_values)),
            'c1_median': float(np.median(c1_values)),
            'c1_std': float(np.std(c1_values)),
            'c1_sem': float(np.std(c1_values) / np.sqrt(len(c1_values))),
            'c1_min': float(np.min(c1_values)),
            'c1_max': float(np.max(c1_values)),
            'bias': float(np.mean(c1_values) - c1_theory),
            'bias_percent': float(abs(np.mean(c1_values) - c1_theory) / c1_theory * 100),
            'all_c1': c1_values.tolist()
        }
        
        # è¯„ä¼°
        print(f"\nğŸ“Š {dimension}ç»´ç»“æœ:")
        print(f"  ç†è®ºc1: {summary['c1_theory']:.6f}")
        print(f"  æå–c1: {summary['c1_mean']:.6f} Â± {summary['c1_sem']:.6f}")
        print(f"  åå·®: {summary['bias']:.6f} ({summary['bias_percent']:.2f}%)")
        print(f"  æˆåŠŸæ ·æœ¬: {summary['successful']}/{n_samples}")
        
        # éªŒè¯æ˜¯å¦æ¥è¿‘ç†è®ºå€¼
        if summary['bias_percent'] < 10:
            status = "âœ… éªŒè¯é€šè¿‡"
        elif summary['bias_percent'] < 20:
            status = "ğŸŸ¡ æ¥è¿‘"
        else:
            status = "âš ï¸ åå·®è¾ƒå¤§"
        
        print(f"  çŠ¶æ€: {status}")
        
        return summary
    
    def run_cross_dimension_test(self, dimensions: list = [3, 4, 5],
                                  n_samples: int = 10) -> dict:
        """
        è¿è¡Œè·¨ç»´åº¦æµ‹è¯•
        
        Args:
            dimensions: è¦æµ‹è¯•çš„ç»´åº¦åˆ—è¡¨
            n_samples: æ¯ç»´åº¦æ ·æœ¬æ•°
            
        Returns:
            æ‰€æœ‰ç»´åº¦çš„ç»“æœ
        """
        print("=" * 70)
        print("è·¨ç»´åº¦éªŒè¯ - c1 = 1/d_f^max")
        print("=" * 70)
        print("\næ ¸å¿ƒçŒœæƒ³éªŒè¯:")
        print("  å¦‚æœ c1 = 1/d_f^max æˆç«‹ï¼Œåˆ™:")
        for d in dimensions:
            print(f"    {d}ç»´ç©ºé—´: c1 = 1/{d} = {1.0/d:.6f}")
        
        all_results = {}
        
        for dim in dimensions:
            result = self.validate_dimension(dim, n_samples=n_samples)
            all_results[f"d{dim}"] = result
        
        # æ±‡æ€»åˆ†æ
        print("\n" + "=" * 70)
        print("ğŸ“‹ è·¨ç»´åº¦æ±‡æ€»")
        print("=" * 70)
        
        print(f"\n{'ç»´åº¦':>6} | {'ç†è®ºc1':>10} | {'æå–c1':>10} | {'åå·®%':>8} | {'çŠ¶æ€':>10}")
        print(f"{'-'*6}-+-{'-'*10}-+-{'-'*10}-+-{'-'*8}-+-{'-'*10}")
        
        valid_results = []
        for dim_key, result in all_results.items():
            if 'error' not in result:
                dim = result['dimension']
                theory = result['c1_theory']
                extracted = result['c1_mean']
                bias_pct = result['bias_percent']
                
                if bias_pct < 10:
                    status = "âœ… é€šè¿‡"
                elif bias_pct < 20:
                    status = "ğŸŸ¡ æ¥è¿‘"
                else:
                    status = "âš ï¸ åå·®"
                
                print(f"{dim:>6} | {theory:>10.6f} | {extracted:>10.6f} | "
                      f"{bias_pct:>8.2f} | {status:>10}")
                
                valid_results.append(result)
        
        # éªŒè¯çº¿æ€§å…³ç³» c1 âˆ 1/d
        if len(valid_results) >= 2:
            print(f"\nğŸ“ˆ çº¿æ€§å…³ç³»éªŒè¯: c1 âˆ 1/d")
            
            dims = np.array([r['dimension'] for r in valid_results])
            c1_means = np.array([r['c1_mean'] for r in valid_results])
            inv_dims = 1.0 / dims
            
            # æ‹Ÿåˆ c1 = k * (1/d)
            # åº”è¯¥ k â‰ˆ 1
            A = np.vstack([inv_dims, np.ones(len(inv_dims))]).T
            k, b = np.linalg.lstsq(A, c1_means, rcond=None)[0]
            
            print(f"  æ‹Ÿåˆ: c1 = {k:.4f} * (1/d) + {b:.6f}")
            print(f"  æ–œç‡k = {k:.4f} (ç†è®ºå€¼ = 1.0)")
            print(f"  æˆªè·b = {b:.6f} (ç†è®ºå€¼ = 0.0)")
            
            # è®¡ç®—R^2
            y_pred = k * inv_dims + b
            ss_res = np.sum((c1_means - y_pred)**2)
            ss_tot = np.sum((c1_means - np.mean(c1_means))**2)
            r_squared = 1 - ss_res / ss_tot if ss_tot > 0 else 0
            print(f"  RÂ² = {r_squared:.4f}")
            
            if abs(k - 1.0) < 0.1 and r_squared > 0.9:
                print(f"\n  ğŸ‰ å¼ºæ”¯æŒ: c1 = 1/d_f^max çŒœæƒ³!")
            elif abs(k - 1.0) < 0.2:
                print(f"\n  ğŸŸ¡ ä¸­ç­‰æ”¯æŒ: éœ€è¦æ›´å¤šæ•°æ®")
            else:
                print(f"\n  âš ï¸ çº¿æ€§å…³ç³»ä¸æ˜æ˜¾")
        
        print("\n" + "=" * 70)
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"../data/cross_dimension_test_{timestamp}.json"
        
        with open(output_file, 'w') as f:
            save_data = {k: {key: val for key, val in v.items() if key != 'all_c1'}
                        for k, v in all_results.items() if 'error' not in v}
            json.dump({
                'timestamp': timestamp,
                'conjecture': 'c1 = 1/d_f^max',
                'results': save_data
            }, f, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")
        
        return all_results


if __name__ == "__main__":
    # è¿è¡Œè·¨ç»´åº¦æµ‹è¯•
    validator = CrossDimensionValidator()
    
    # æµ‹è¯•3, 4, 5ç»´
    results = validator.run_cross_dimension_test(
        dimensions=[3, 4, 5],
        n_samples=8  # æ¯ç»´åº¦æ ·æœ¬æ•°
    )
    
    print("\n" + "=" * 70)
    print("æç¤º: ä½¿ç”¨æ›´å¤šæ ·æœ¬(n_samples=20+)è·å¾—æ›´ç²¾ç¡®ç»“æœ")
    print("=" * 70)
