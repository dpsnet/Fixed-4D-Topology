"""
é‡æ–°è®¾è®¡çš„c1æå–å™¨
Revamped C1 Extractor

é…åˆæ–°çš„åˆ†å½¢ç”Ÿæˆå™¨
å…³é”®æ”¹è¿›:
1. é’ˆå¯¹æ–°çš„è°±ç»´åº¦å…¬å¼ä¼˜åŒ–
2. ä½¿ç”¨æ­£ç¡®çš„æ‹Ÿåˆå‡½æ•°
3. æ›´å¥½çš„é”™è¯¯å¤„ç†
"""

import numpy as np
from typing import Dict, Tuple
from scipy.optimize import curve_fit


class RevampedExtractor:
    """
    é‡æ–°è®¾è®¡çš„c1æå–å™¨
    
    é’ˆå¯¹æ–°çš„å®‰å…¨è°±ç»´åº¦å…¬å¼:
    d_s(â„“) = d_min + (d_max - d_min) / (1 + (â„“_0/â„“)^(1/c1))
    """
    
    def __init__(self, dimension: int):
        self.d = dimension
        self.d_min = 2.0
        self.d_max = float(dimension)
        self.c1_theory = 1.0 / dimension
    
    def extract_c1(self, points: np.ndarray, ell_0: float = 1.0) -> Dict:
        """
        æå–c1
        
        æ­¥éª¤:
        1. æ„å»ºå›¾æ‹‰æ™®æ‹‰æ–¯
        2. è®¡ç®—è°±ç»´åº¦
        3. æ‹Ÿåˆæå–c1å’Œd_max
        """
        try:
            from fractal_laplacian import FractalLaplacian
            
            # æ„å»ºæ‹‰æ™®æ‹‰æ–¯
            fl = FractalLaplacian(dimension=self.d)
            L = fl.construct_graph_laplacian(points, epsilon=None)
            
            # è®¡ç®—è°±ç»´åº¦
            # ä½¿ç”¨è¾ƒå®½çš„tèŒƒå›´
            t_range = np.logspace(-2, 2, 60)
            t_vals, d_s = fl.compute_spectral_dimension(
                L, 
                t_range=t_range,
                n_eigenvalues=min(50, len(points)//4)
            )
            
            # è½¬æ¢ä¸ºé•¿åº¦å°ºåº¦
            ell_vals = np.sqrt(t_vals)
            
            # è¿‡æ»¤æœ‰æ•ˆå€¼
            valid = (ell_vals > 0) & (d_s > 0.5) & (d_s < self.d + 1) & \
                    (~np.isnan(ell_vals)) & (~np.isnan(d_s))
            
            if np.sum(valid) < 10:
                return {
                    'error': f'Insufficient valid points: {np.sum(valid)}',
                    'd_s_range': (float(d_s.min()), float(d_s.max()))
                }
            
            ell = ell_vals[valid]
            d_s_valid = d_s[valid]
            
            # æ–¹æ³•1: éçº¿æ€§æ‹Ÿåˆå®Œæ•´å…¬å¼
            result_fit = self._fit_full_formula(ell, d_s_valid, ell_0)
            
            # æ–¹æ³•2: çº¿æ€§åŒ–æ‹Ÿåˆ
            result_linear = self._fit_linearized(ell, d_s_valid, ell_0)
            
            # é€‰æ‹©æ›´å¥½çš„ç»“æœ
            if result_fit.get('quality', 0) > result_linear.get('quality', 0):
                result = result_fit
                result['method_used'] = 'nonlinear'
            else:
                result = result_linear
                result['method_used'] = 'linearized'
            
            # æ·»åŠ é¢å¤–ä¿¡æ¯
            result['d_s_range'] = (float(d_s.min()), float(d_s.max()))
            result['ell_range'] = (float(ell.min()), float(ell.max()))
            result['n_points_used'] = int(np.sum(valid))
            
            return result
            
        except Exception as e:
            return {'error': str(e)}
    
    def _fit_full_formula(self, ell: np.ndarray, d_s: np.ndarray, 
                          ell_0: float) -> Dict:
        """
        éçº¿æ€§æ‹Ÿåˆå®Œæ•´å…¬å¼
        
        d_s = d_min + (d_max - d_min) / (1 + (ell_0/ell)^(1/c1))
        
        æ‹Ÿåˆå‚æ•°: d_max, c1
        """
        def model(ell, d_max, c1):
            if c1 <= 0:
                return np.full_like(ell, np.nan)
            ratio = ell_0 / ell
            exponent = 1.0 / c1
            transition = 1.0 / (1.0 + ratio ** exponent)
            return self.d_min + (d_max - self.d_min) * transition
        
        # åˆå§‹çŒœæµ‹
        p0 = [self.d_max, self.c1_theory]
        
        try:
            # è¾¹ç•Œçº¦æŸ
            bounds = ([self.d_min, 0.01], [self.d_max + 2, 2.0])
            
            popt, pcov = curve_fit(model, ell, d_s, p0=p0, bounds=bounds, maxfev=5000)
            
            d_max_fit, c1_fit = popt
            
            # è®¡ç®—R^2
            y_pred = model(ell, *popt)
            ss_res = np.sum((d_s - y_pred)**2)
            ss_tot = np.sum((d_s - np.mean(d_s))**2)
            r_squared = 1 - ss_res / ss_tot if ss_tot > 0 else 0
            
            # å‚æ•°è¯¯å·®
            perr = np.sqrt(np.diag(pcov))
            
            return {
                'c1': float(c1_fit),
                'c1_error': float(perr[1]),
                'c1_theory': self.c1_theory,
                'd_max': float(d_max_fit),
                'd_max_error': float(perr[0]),
                'd_max_theory': self.d_max,
                'quality': float(r_squared),
                'bias': float(c1_fit - self.c1_theory),
                'bias_percent': float(abs(c1_fit - self.c1_theory) / self.c1_theory * 100),
            }
            
        except Exception as e:
            return {'error': f'Nonlinear fit failed: {e}'}
    
    def _fit_linearized(self, ell: np.ndarray, d_s: np.ndarray, 
                        ell_0: float) -> Dict:
        """
        çº¿æ€§åŒ–æ‹Ÿåˆ
        
        ä»å…¬å¼:
        d_s = d_min + (d_max - d_min) / (1 + (ell_0/ell)^(1/c1))
        
        å˜å½¢:
        (d_max - d_min) / (d_s - d_min) = 1 + (ell_0/ell)^(1/c1)
        (d_max - d_min) / (d_s - d_min) - 1 = (ell_0/ell)^(1/c1)
        
        å–å¯¹æ•°:
        ln[(d_max - d_min)/(d_s - d_min) - 1] = (1/c1) * ln(ell_0/ell)
        
        è¿™å…³äº 1/c1 æ˜¯çº¿æ€§çš„ï¼
        """
        try:
            # éœ€è¦d_maxçš„ä¼°è®¡
            # ä½¿ç”¨d_sçš„æœ€å¤§å€¼ä½œä¸ºåˆå§‹ä¼°è®¡
            d_max_init = min(np.max(d_s) * 1.1, self.d_max + 1)
            
            # è¿­ä»£ä¼˜åŒ–
            best_result = None
            best_quality = -1
            
            for d_max_try in np.linspace(max(np.max(d_s), self.d_min + 0.5), 
                                          self.d_max + 1, 10):
                y = (d_max_try - self.d_min) / (d_s - self.d_min) - 1
                
                # è¿‡æ»¤æ­£å€¼
                valid_y = y > 0
                if np.sum(valid_y) < 5:
                    continue
                
                ell_valid = ell[valid_y]
                y_valid = y[valid_y]
                
                # è®¡ç®—å¯¹æ•°
                log_y = np.log(y_valid)
                log_ratio = np.log(ell_0 / ell_valid)
                
                # çº¿æ€§æ‹Ÿåˆ: log_y = (1/c1) * log_ratio
                # å³: y = slope * x, å…¶ä¸­ slope = 1/c1
                
                slope = np.sum(log_y * log_ratio) / np.sum(log_ratio ** 2)
                
                if slope > 0:
                    c1_fit = 1.0 / slope
                    
                    # è®¡ç®—é¢„æµ‹å€¼
                    y_pred = slope * log_ratio
                    ss_res = np.sum((log_y - y_pred)**2)
                    ss_tot = np.sum((log_y - np.mean(log_y))**2)
                    r_squared = 1 - ss_res / ss_tot if ss_tot > 0 else 0
                    
                    if r_squared > best_quality:
                        best_quality = r_squared
                        best_result = {
                            'c1': float(c1_fit),
                            'c1_theory': self.c1_theory,
                            'd_max': float(d_max_try),
                            'd_max_theory': self.d_max,
                            'quality': float(r_squared),
                            'bias': float(c1_fit - self.c1_theory),
                            'bias_percent': float(abs(c1_fit - self.c1_theory) / self.c1_theory * 100),
                        }
            
            return best_result if best_result else {'error': 'Linearized fit failed'}
            
        except Exception as e:
            return {'error': f'Linearized fit failed: {e}'}


def test_revamped_extraction():
    """æµ‹è¯•é‡æ–°è®¾è®¡çš„æå–å™¨"""
    print("=" * 70)
    print("é‡æ–°è®¾è®¡çš„c1æå–å™¨ - æµ‹è¯•")
    print("=" * 70)
    
    from revamped_fractal_generator import RevampedFractalGenerator
    
    for dim in [3, 4, 5]:
        print(f"\n{'='*70}")
        print(f"{dim}ç»´ç©ºé—´ (ç†è®ºc1 = 1/{dim} = {1.0/dim:.4f})")
        print(f"{'='*70}")
        
        gen = RevampedFractalGenerator(dimension=dim)
        extractor = RevampedExtractor(dimension=dim)
        
        c1_values = []
        
        for i in range(5):
            np.random.seed(i * 10 + dim)
            
            # ç”Ÿæˆä¸¤ç§ç±»å‹çš„åˆ†å½¢
            if i % 2 == 0:
                points, _, _ = gen.generate_shell_based_fractal(n_points=400)
            else:
                points, _ = gen.generate_explicit_transition(n_points=400)
            
            # æå–c1
            result = extractor.extract_c1(points)
            
            if 'c1' in result:
                c1_values.append(result['c1'])
                bias = result.get('bias_percent', 0)
                method = result.get('method_used', '?')
                d_max = result.get('d_max', 0)
                quality = result.get('quality', 0)
                
                status = 'âœ“' if bias < 20 else ('~' if bias < 50 else 'âœ—')
                print(f"  æ ·æœ¬{i+1}: c1={result['c1']:.4f}, d_max={d_max:.2f}, "
                      f"RÂ²={quality:.3f}, åå·®={bias:.1f}% [{method}] {status}")
            else:
                print(f"  æ ·æœ¬{i+1}: å¤±è´¥ - {result.get('error', 'Unknown')[:40]}")
        
        if c1_values:
            c1_values = np.array(c1_values)
            mean_c1 = np.mean(c1_values)
            theory = 1.0 / dim
            bias_pct = abs(mean_c1 - theory) / theory * 100
            
            print(f"\n  ç»Ÿè®¡:")
            print(f"    ç†è®ºc1: {theory:.4f}")
            print(f"    æå–å‡å€¼: {mean_c1:.4f} Â± {np.std(c1_values):.4f}")
            print(f"    åå·®: {bias_pct:.1f}%")
            
            if bias_pct < 20:
                print(f"    âœ… è¾¾åˆ°ç›®æ ‡ (<20%)")
            elif bias_pct < 50:
                print(f"    ğŸŸ¡ æ¥è¿‘ç›®æ ‡")
            else:
                print(f"    âš ï¸ éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›")
    
    print("\n" + "=" * 70)


if __name__ == "__main__":
    test_revamped_extraction()
