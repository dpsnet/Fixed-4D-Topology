"""
è°±ç»´åº¦æµéªŒè¯è„šæœ¬
Validation of Spectral Dimension Flow in Generated Fractals

ç›®æ ‡: ä½¿ç”¨æ”¹è¿›çš„åˆ†å½¢ç”Ÿæˆå™¨éªŒè¯ c1 = 1/4 æå–ç²¾åº¦
"""

import numpy as np
from spectral_flow_fractal import SpectralFlowFractal
from fractal_laplacian import FractalLaplacian
import json
from datetime import datetime


class SpectralFlowValidator:
    """
    è°±ç»´åº¦æµéªŒè¯å™¨
    
    éªŒè¯ç”Ÿæˆçš„åˆ†å½¢æ˜¯å¦å…·æœ‰æ­£ç¡®çš„è°±ç»´åº¦æµç‰¹å¾
    """
    
    def __init__(self, true_c1: float = 0.25):
        self.true_c1 = true_c1
        
    def validate_fractal(self, points: np.ndarray, 
                         method: str = 'laplacian') -> dict:
        """
        éªŒè¯å•ä¸ªåˆ†å½¢æ ·æœ¬
        
        Returns:
            éªŒè¯ç»“æžœå­—å…¸
        """
        fl = FractalLaplacian(dimension=points.shape[1])
        
        # æž„å»ºæ‹‰æ™®æ‹‰æ–¯
        try:
            L = fl.construct_graph_laplacian(points, epsilon=None)
        except Exception as e:
            return {'error': f'Laplacian failed: {e}'}
        
        # è®¡ç®—è°±ç»´åº¦
        try:
            t_vals, d_s = fl.compute_spectral_dimension(
                L, n_eigenvalues=min(50, len(points)//4)
            )
        except Exception as e:
            return {'error': f'Spectral dimension failed: {e}'}
        
        # æ£€æŸ¥è°±ç»´åº¦èŒƒå›´
        d_s_min = np.min(d_s)
        d_s_max = np.max(d_s)
        d_s_range = d_s_max - d_s_min
        
        # æå–c1
        result = fl.extract_c1_from_spectral_flow(t_vals, d_s)
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        result['d_s_min'] = float(d_s_min)
        result['d_s_max'] = float(d_s_max)
        result['d_s_range'] = float(d_s_range)
        result['expected_range'] = 2.0  # æœŸæœ›ä»Ž4åˆ°2
        result['range_quality'] = min(d_s_range / 2.0, 1.0)
        
        return result
    
    def batch_validate(self, fractal_generator: SpectralFlowFractal,
                       n_samples: int = 50,
                       n_points: int = 300,
                       fractal_type: str = 'layered') -> dict:
        """
        æ‰¹é‡éªŒè¯åˆ†å½¢æ ·æœ¬
        
        Args:
            fractal_generator: åˆ†å½¢ç”Ÿæˆå™¨
            n_samples: æ ·æœ¬æ•°é‡
            n_points: æ¯æ ·æœ¬ç‚¹æ•°
            fractal_type: åˆ†å½¢ç±»åž‹
            
        Returns:
            ç»Ÿè®¡éªŒè¯ç»“æžœ
        """
        print(f"æ‰¹é‡éªŒè¯: {n_samples} æ ·æœ¬ x {n_points} ç‚¹ ({fractal_type})")
        
        results = []
        
        for i in range(n_samples):
            if i % 10 == 0:
                print(f"  è¿›åº¦: {i}/{n_samples}")
            
            # ç”Ÿæˆä¸åŒç±»åž‹çš„åˆ†å½¢
            if fractal_type == 'layered':
                points, _, _ = fractal_generator.generate_layered_fractal(n_points)
            elif fractal_type == 'ifs':
                points = fractal_generator.generate_ifs_fractal(n_points)
            elif fractal_type == 'smooth':
                points = fractal_generator.generate_dimension_transition_fractal(
                    n_points, transition_type='smooth'
                )
            elif fractal_type == 'quantum':
                points = fractal_generator.generate_quantum_spacetime_fractal(n_points)
            else:
                points = fractal_generator.generate_layered_fractal(n_points)
            
            # éªŒè¯
            result = self.validate_fractal(points)
            
            if 'error' not in result and not np.isnan(result.get('c1', np.nan)):
                results.append(result)
        
        # ç»Ÿè®¡åˆ†æž
        if len(results) == 0:
            return {'error': 'No valid results'}
        
        c1_values = [r['c1'] for r in results if 'c1' in r]
        quality_values = [r['quality'] for r in results if 'quality' in r]
        range_values = [r['d_s_range'] for r in results if 'd_s_range' in r]
        
        c1_values = np.array(c1_values)
        
        # è¿‡æ»¤å¼‚å¸¸å€¼ (c1åº”åœ¨åˆç†èŒƒå›´)
        valid_mask = (c1_values > 0) & (c1_values < 1.0)
        valid_c1 = c1_values[valid_mask]
        
        if len(valid_c1) == 0:
            return {'error': 'No valid c1 values'}
        
        summary = {
            'fractal_type': fractal_type,
            'n_samples': n_samples,
            'successful': len(valid_c1),
            'mean_c1': float(np.mean(valid_c1)),
            'std_c1': float(np.std(valid_c1)),
            'sem_c1': float(np.std(valid_c1) / np.sqrt(len(valid_c1))),
            'min_c1': float(np.min(valid_c1)),
            'max_c1': float(np.max(valid_c1)),
            'true_c1': self.true_c1,
            'bias': float(np.mean(valid_c1) - self.true_c1),
            'mean_quality': float(np.mean(quality_values)) if quality_values else 0,
            'mean_d_s_range': float(np.mean(range_values)) if range_values else 0,
            'all_c1': valid_c1.tolist()
        }
        
        return summary
    
    def print_validation_report(self, summary: dict):
        """æ‰“å°éªŒè¯æŠ¥å‘Š"""
        print("\n" + "=" * 70)
        print(f"è°±ç»´åº¦æµéªŒè¯æŠ¥å‘Š - {summary.get('fractal_type', 'Unknown')}")
        print("=" * 70)
        
        if 'error' in summary:
            print(f"\nâŒ é”™è¯¯: {summary['error']}")
            return
        
        print(f"\nðŸ“Š æ ·æœ¬ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬: {summary['n_samples']}")
        print(f"   æˆåŠŸéªŒè¯: {summary['successful']}")
        
        print(f"\nðŸŽ¯ c1 æå–ç»“æžœ:")
        print(f"   ç†è®ºå€¼: {summary['true_c1']:.6f}")
        print(f"   æå–å‡å€¼: {summary['mean_c1']:.6f}")
        print(f"   æ ‡å‡†å·®: {summary['std_c1']:.6f}")
        print(f"   æ ‡å‡†è¯¯: {summary['sem_c1']:.6f}")
        print(f"   èŒƒå›´: [{summary['min_c1']:.6f}, {summary['max_c1']:.6f}]")
        
        print(f"\nðŸ“ˆ è´¨é‡è¯„ä¼°:")
        print(f"   åå·® (æå–-ç†è®º): {summary['bias']:.6f}")
        print(f"   å¹³å‡æ‹Ÿåˆè´¨é‡: {summary['mean_quality']:.4f}")
        print(f"   å¹³å‡è°±ç»´åº¦èŒƒå›´: {summary['mean_d_s_range']:.2f}")
        
        # è¯„ä¼°
        bias_pct = abs(summary['bias']) / summary['true_c1'] * 100
        if bias_pct < 10:
            grade = "âœ… ä¼˜ç§€"
        elif bias_pct < 20:
            grade = "ðŸŸ¡ è‰¯å¥½"
        else:
            grade = "ðŸ”´ éœ€æ”¹è¿›"
        
        print(f"\n{grade} - åå·®: {bias_pct:.1f}%")
        
        print("=" * 70)


def run_comprehensive_validation():
    """è¿è¡Œç»¼åˆéªŒè¯"""
    print("=" * 70)
    print("è°±ç»´åº¦æµåˆ†å½¢ç»¼åˆéªŒè¯")
    print("ç›®æ ‡: éªŒè¯æ”¹è¿›çš„åˆ†å½¢ç”Ÿæˆå™¨èƒ½å¦å‡†ç¡®æå– c1 = 0.25")
    print("=" * 70)
    
    validator = SpectralFlowValidator(true_c1=0.25)
    sff = SpectralFlowFractal(dimension=4, c1=0.25)
    
    fractal_types = ['layered', 'ifs', 'smooth', 'quantum']
    all_results = {}
    
    for ft in fractal_types:
        print(f"\n{'='*70}")
        print(f"éªŒè¯åˆ†å½¢ç±»åž‹: {ft}")
        print(f"{'='*70}")
        
        summary = validator.batch_validate(
            sff, n_samples=30, n_points=250, fractal_type=ft
        )
        
        validator.print_validation_report(summary)
        all_results[ft] = summary
    
    # æ±‡æ€»æŠ¥å‘Š
    print("\n" + "=" * 70)
    print("æ±‡æ€»æŠ¥å‘Š")
    print("=" * 70)
    
    print(f"\n{'åˆ†å½¢ç±»åž‹':<12} | {'c1å‡å€¼':>10} | {'åå·®':>10} | {'è´¨é‡':>8}")
    print(f"{'-'*12}-+-{'-'*10}-+-{'-'*10}-+-{'-'*8}")
    
    for ft, result in all_results.items():
        if 'error' not in result:
            print(f"{ft:<12} | {result['mean_c1']:10.4f} | "
                  f"{result['bias']:10.4f} | {result['mean_quality']:8.3f}")
    
    # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
    valid_results = {k: v for k, v in all_results.items() 
                     if 'error' not in v}
    
    if valid_results:
        best_method = min(valid_results.items(), 
                         key=lambda x: abs(x[1]['bias']))
        print(f"\nâœ… æœ€ä½³æ–¹æ³•: {best_method[0]}")
        print(f"   c1 = {best_method[1]['mean_c1']:.4f} Â± {best_method[1]['sem_c1']:.4f}")
        print(f"   åå·®: {best_method[1]['bias']:.4f}")
    
    print("\n" + "=" * 70)
    
    # ä¿å­˜ç»“æžœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"../data/validation_results_{timestamp}.json"
    
    with open(output_file, 'w') as f:
        # è¿‡æ»¤æŽ‰numpyæ•°ç»„
        save_results = {}
        for k, v in all_results.items():
            if 'error' not in v:
                save_results[k] = {key: val for key, val in v.items() 
                                  if key != 'all_c1'}
        
        json.dump({
            'timestamp': timestamp,
            'true_c1': 0.25,
            'results': save_results
        }, f, indent=2)
    
    print(f"\nç»“æžœå·²ä¿å­˜: {output_file}")
    
    return all_results


if __name__ == "__main__":
    results = run_comprehensive_validation()
