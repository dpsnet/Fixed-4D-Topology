# K1.3: 神经网络维度的训练动态

## 1. 引言

神经网络的有效维度在训练过程中动态演化。本章建立描述这种演化的动力学方程。

## 2. 维度演化方程

### 2.1 主方程

**假设**: 有效维度 $d_{\text{eff}}(t)$ 的演化取决于：
- 当前维度 $d_{\text{eff}}$
- 损失 landscape 的曲率
- 学习率
- 数据复杂度

**维度演化主方程**:

$$\frac{d d_{\text{eff}}}{d t} = \alpha \cdot \mathcal{L}(t) \cdot (d_{\text{data}} - d_{\text{eff}}) - \beta \cdot d_{\text{eff}} \cdot R(t)$$

其中:
- $\alpha$: 学习系数
- $\mathcal{L}(t)$: 训练损失
- $d_{\text{data}}$: 数据内在维度
- $\beta$: 正则化系数  
- $R(t)$: 正则化强度

### 2.2 物理意义

**第一项** ($\alpha \mathcal{L} (d_{\text{data}} - d_{\text{eff}})$):
- 当 $d_{\text{eff}} < d_{\text{data}}$: 维度增长（学习数据结构）
- 当 $d_{\text{eff}} > d_{\text{data}}$: 维度减小（过度拟合修正）
- 损失 $\mathcal{L}$ 大时增长快

**第二项** ($-\beta d_{\text{eff}} R$):
- 正则化倾向于降低维度
- 抑制冗余参数

### 2.3 稳态分析

令 $\frac{d d_{\text{eff}}}{d t} = 0$:

$$d_{\text{eff}}^* = \frac{\alpha \mathcal{L}^* d_{\text{data}}}{\alpha \mathcal{L}^* + \beta R^*}$$

其中 $\mathcal{L}^*, R^*$ 是稳态值。

**极限情况**:
- 无正则化 ($\beta = 0$): $d_{\text{eff}}^* = d_{\text{data}}$
- 强正则化 ($\beta R^* \gg \alpha \mathcal{L}^*$): $d_{\text{eff}}^* \ll d_{\text{data}}$

## 3. 基于Fisher信息的推导

### 3.1 Fisher矩阵的时间演化

Fisher信息矩阵 $F(t)$ 随训练演化。对于SGD:

$$\frac{d F}{d t} = -\eta \nabla_\theta \mathcal{L} \cdot \nabla_\theta \mathcal{L}^T + \text{noise}$$

### 3.2 特征值动力学

假设特征值 $\lambda_i(t)$ 独立演化:

$$\frac{d \lambda_i}{d t} = a_i \lambda_i - b_i \lambda_i^2$$

其中:
- 第一项: 指数增长（学习敏感方向）
- 第二项: 饱和（方向已充分学习）

**解**:
$$\lambda_i(t) = \frac{a_i \lambda_i(0)}{b_i \lambda_i(0) + (a_i - b_i \lambda_i(0)) e^{-a_i t}}$$

### 3.3 维度演化的显式形式

$$d_{\text{eff}}(t) = \frac{\left(\sum_i \lambda_i(t)\right)^2}{\sum_i \lambda_i(t)^2}$$

**三阶段行为**:

| 阶段 | 时间 | 特征 | $d_{\text{eff}}$ 行为 |
|------|------|------|----------------------|
| 早期 | $t \ll 1/a$ | 所有 $\lambda_i$ 增长 | 快速增长 |
| 中期 | $t \sim 1/a$ | 大 $\lambda_i$ 饱和 | 平台期 |
| 晚期 | $t \gg 1/a$ | 正则化主导 | 缓慢下降 |

## 4. 与SGD动力学的联系

### 4.1 SGD的连续极限

SGD在连续极限下:

$$d\theta = -\nabla \mathcal{L} dt + \sqrt{\eta \Sigma} dW$$

其中 $\Sigma$ 是梯度协方差。

### 4.2 维度-损失关系

**定理K.3.1**: 在适当的假设下:

$$\frac{d d_{\text{eff}}}{d \mathcal{L}} = -\frac{\gamma}{\mathcal{L}}$$

其中 $\gamma$ 是数据相关的常数。

**推论**: 对数线性关系
$$d_{\text{eff}}(t) = d_{\text{eff}}^{\text{init}} - \gamma \ln\left(\frac{\mathcal{L}(t)}{\mathcal{L}_{\text{init}}}\right)$$

### 4.3 学习率影响

**定理K.3.2**: 最优学习率与维度相关:

$$\eta^* \propto \frac{1}{d_{\text{eff}}}$$

**解释**: 高维度需要更小学习率（更复杂的 landscape）。

## 5. 分阶段动态

### 5.1 早期阶段（$t < t_1$）

**特征**: 损失快速下降，维度快速增长

**近似**:
$$d_{\text{eff}}(t) \approx d_{\text{eff}}^{\text{init}} + c_1 t$$

**物理**: 网络学习主要数据结构

### 5.2 中期阶段（$t_1 < t < t_2$）

**特征**: 损失下降放缓，维度平台

**近似**:
$$d_{\text{eff}}(t) \approx d_{\text{data}}$$

**物理**: 维度与数据复杂度匹配

### 5.3 晚期阶段（$t > t_2$）

**特征**: 损失平稳，维度缓慢下降

**近似**:
$$d_{\text{eff}}(t) \approx d_{\text{data}} - c_2 \ln(t)$$

**物理**: 正则化和噪声逐渐简化网络

## 6. 临界点与相变

### 6.1 维度相变

当数据复杂度变化时，$d_{\text{eff}}$ 可能出现相变:

$$d_{\text{eff}} \sim |d_{\text{data}} - d_c|^{-\nu}$$

其中 $d_c$ 是临界维度，$\nu$ 是临界指数。

### 6.2 泛化相变

泛化误差在 $d_{\text{eff}} = d_{\text{data}}$ 处有最小值:

$$\text{GenError} = \begin{cases}
\text{high} & d_{\text{eff}} < d_{\text{data}} \text{ (欠拟合)} \\
\text{low} & d_{\text{eff}} \approx d_{\text{data}} \text{ (最优)} \\
\text{high} & d_{\text{eff}} > d_{\text{data}} \text{ (过拟合)}
\end{cases}$$

## 7. 与Dimensionics主方程的统一

### 7.1 对应关系

| Dimensionics | 神经网络训练 |
|-------------|-------------|
| 能量尺度 $\mu$ | 训练时间 $t$ |
| 谱维度 $d_s(\mu)$ | 有效维度 $d_{\text{eff}}(t)$ |
| Beta函数 $\beta(d)$ | 演化函数 $f(d, \mathcal{L})$ |
| UV固定点 | 早训练期 |
| IR固定点 | 收敛期 |

### 7.2 统一方程

$$t \frac{\partial d_{\text{eff}}}{\partial t} = \beta_{\text{NN}}(d_{\text{eff}}, \mathcal{L})$$

其中Beta函数:
$$\beta_{\text{NN}}(d, \mathcal{L}) = \alpha \mathcal{L} (d_{\text{data}} - d) - \beta d R$$

## 8. 实验预测

### 8.1 预测1: 三阶段曲线

测量 $d_{\text{eff}}(t)$ 应显示:
1. 初始快速增长
2. 中期平台
3. 晚期缓慢下降

### 8.2 预测2: 对数线性关系

$\ln \mathcal{L}$ vs $d_{\text{eff}}$ 应为线性关系。

### 8.3 预测3: 最优维度匹配

最优泛化发生在 $d_{\text{eff}} \approx d_{\text{data}}$。

## 9. 数值模拟

### 9.1 简化模型

考虑特征值的平均场近似:

$$\frac{d \bar{\lambda}}{d t} = a \bar{\lambda} - b \bar{\lambda}^2$$

解:
$$\bar{\lambda}(t) = \frac{a/b}{1 + e^{-a(t-t_0)}}$$

### 9.2 模拟结果

（待实验验证）

## 10. 结论

神经网络维度的训练动态遵循可描述的演化方程，与物理中的重整化群有深刻类比。这为理解深度学习提供了新的理论视角。

**关键洞察**: 训练是一个维度"流动"过程，最终稳定在数据复杂度附近。

---

**严格性**: L2 (理论框架+可验证预测)  
**下一步**: K1.4 泛化界证明
