# K1.2: 神经网络有效维度的定义与性质

## 1. 引言

有效维度是衡量神经网络"真实"复杂度的核心指标，反映了参数空间中的独立自由度数量。

## 2. 有效维度的定义

### 2.1 基于Fisher信息的定义

**定义K.2.1 (Fisher有效维度)**: 对于具有参数 $\theta \in \mathbb{R}^D$ 和Fisher信息矩阵 $F(\theta)$ 的神经网络，其有效维度定义为：

$$d_{\text{eff}}^{(F)} = \frac{(\text{tr} F)^2}{\text{tr}(F^2)} = \frac{\left(\sum_{i=1}^D \lambda_i\right)^2}{\sum_{i=1}^D \lambda_i^2}$$

其中 $\lambda_i$ 是 $F$ 的特征值。

### 2.2 等价形式

**参与比形式**: 令 $p_i = \lambda_i / \sum_j \lambda_j$，则
$$d_{\text{eff}} = \left(\sum_{i=1}^D p_i^2\right)^{-1}$$

这表示特征值分布的"参与比"——有多少特征值显著贡献。

### 2.3 几何解释

$d_{\text{eff}}$ 表示参数空间中损失 landscape 的"有效曲率方向"数量：
- 每个大特征值对应一个敏感方向
- 小特征值对应平坦方向（冗余）

## 3. 基本性质

### 3.1 范围约束

**引理K.2.1**: $1 \leq d_{\text{eff}} \leq D$

*证明*:
- 下界：由柯西-施瓦茨不等式，$(\sum \lambda_i)^2 \leq D \sum \lambda_i^2$，因此 $d_{\text{eff}} \leq D$
- 上界：$\sum p_i^2 \leq (\sum p_i)^2 = 1$，因此 $d_{\text{eff}} \geq 1$

**等号条件**:
- $d_{\text{eff}} = D$: 当且仅当 $F \propto I$（各向同性）
- $d_{\text{eff}} = 1$: 当且仅当只有一个非零特征值

### 3.2 单调性

**引理K.2.2**: 若 $F_1 \preceq F_2$（半正定序），则 $d_{\text{eff}}(F_1) \leq d_{\text{eff}}(F_2)$。

### 3.3 尺度不变性

**引理K.2.3**: $d_{\text{eff}}$ 对Fisher矩阵的缩放不变：
$$d_{\text{eff}}(\alpha F) = d_{\text{eff}}(F), \quad \forall \alpha > 0$$

## 4. 其他有效维度定义

### 4.1 基于特征值阈值的定义

**定义K.2.2 (截断维度)**: 给定阈值 $\epsilon > 0$:
$$d_{\text{eff}}^{(\epsilon)} = \#\{i : \lambda_i > \epsilon \lambda_{\max}\}$$

### 4.2 基于熵的定义

**定义K.2.3 (von Neumann维度)**: 
$$d_{\text{eff}}^{(vN)} = \exp\left(-\sum_i p_i \ln p_i\right)$$

**定理K.2.1**: $d_{\text{eff}}^{(vN)} \leq d_{\text{eff}}^{(F)}$

*证明*: 由Jensen不等式，熵是凹函数...

### 4.3 基于累积方差的定义

**定义K.2.4 (PCA维度)**: 给定解释方差比例 $\eta \in (0,1)$:
$$d_{\text{eff}}^{(PCA)}(\eta) = \min\left\{k : \frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^D \lambda_i} \geq \eta\right\}$$

## 5. 神经网络的特殊性质

### 5.1 参数冗余

**观察**: 对于过参数化神经网络，通常 $d_{\text{eff}} \ll D$。

**解释**: 
- 对称性（如神经元置换）产生零特征值
- 相关性导致小特征值

### 5.2 层间差异

不同层的有效维度贡献不同：
- 输入层：通常 $d_{\text{eff}} \approx$ 输入维度
- 隐藏层：取决于架构
- 输出层：通常 $d_{\text{eff}} \approx$ 输出维度

**分层有效维度**:
$$d_{\text{eff}}^{\text{total}} = \sum_{l=1}^L d_{\text{eff}}^{(l)}$$

### 5.3 与架构的关系

| 架构 | 预期 $d_{\text{eff}}/D$ | 原因 |
|------|------------------------|------|
| 全连接 | 中等 (~0.3-0.5) | 参数密集 |
| CNN | 较低 (~0.2-0.4) | 权重共享 |
| ResNet | 较高 (~0.4-0.6) | 残差连接增加自由度 |
| Transformer | 取决于头数 | 注意力稀疏性 |

## 6. 计算算法

### 6.1 精确计算

```python
def compute_effective_dimension(F):
    """
    Compute Fisher effective dimension.
    
    Args:
        F: Fisher information matrix (D x D)
    
    Returns:
        d_eff: Effective dimension
    """
    trace_F = np.trace(F)
    trace_F2 = np.trace(F @ F)
    d_eff = (trace_F ** 2) / trace_F2
    return d_eff
```

### 6.2 近似计算（大规模）

对于 $D \gg 1$，直接计算 $F$ 不可行。

**随机迹估计**:
$$\text{tr}(F) \approx \frac{1}{M} \sum_{m=1}^M v_m^T F v_m$$

其中 $v_m \sim \mathcal{N}(0, I)$。

** Hutchinson trick**:
$$\text{tr}(F^2) \approx \frac{1}{M} \sum_{m=1}^M v_m^T F^2 v_m$$

### 6.3 特征值抽样

使用随机SVD估计前 $k$ 个特征值：
$$d_{\text{eff}} \approx \frac{(\sum_{i=1}^k \lambda_i)^2}{\sum_{i=1}^k \lambda_i^2}$$

## 7. 理论界

### 7.1 上界

**定理K.2.2**: 对于深度 $L$、每层宽度 $n$ 的网络：
$$d_{\text{eff}} \leq \min(D, n_{\text{data}} \cdot L)$$

其中 $n_{\text{data}}$ 是训练样本数。

### 7.2 下界

**定理K.2.3**: 假设网络能实现 $K$ 个线性无关的函数：
$$d_{\text{eff}} \geq K - 1$$

### 7.3 泛化误差界

**定理K.2.4 (维度-样本关系)**: 以高概率，
$$|R - \hat{R}| \lesssim \sqrt{\frac{d_{\text{eff}} \ln(D/d_{\text{eff}})}{N}}$$

其中 $R$ 是风险，$\hat{R}$ 是经验风险，$N$ 是样本数。

## 8. 与PAC-Bayes的联系

**PAC-Bayes界**:
$$\mathbb{E}[R] \leq \hat{R} + \sqrt{\frac{\text{KL}(Q\|P) + \ln(1/\delta)}{2N}}$$

其中 $Q$ 是后验，$P$ 是先验。

**联系**: KL散度与Fisher信息相关，因此：
$$\text{KL}(Q\|P) \approx \frac{1}{2} d_{\text{eff}}$$

## 9. 结论

有效维度 $d_{\text{eff}}$ 提供了一个量化神经网络本质复杂度的框架，连接了：
- 信息几何
- 统计学习理论
- 模型选择

**关键洞察**: 神经网络的表达能力由 $d_{\text{eff}}$ 而非 $D$ 决定。

---

**严格性**: L1 (定义严格，定理有证明)  
**下一步**: K1.3 训练动态方程
