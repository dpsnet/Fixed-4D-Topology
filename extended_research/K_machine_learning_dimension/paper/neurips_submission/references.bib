% Neural Network Generalization and Capacity
@article{zhang2017understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Sam and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={International Conference on Learning Representations (ICLR)},
  year={2017}
}

@article{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ball, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua},
  journal={International Conference on Machine Learning (ICML)},
  pages={233--242},
  year={2017}
}

@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={30},
  year={2017}
}

@article{neyshabur2018towards,
  title={Towards understanding the role of over-parametrization in generalization of neural networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={International Conference on Learning Representations (ICLR)},
  year={2018}
}

% Fisher Information and Natural Gradient
@book{amari2016information,
  title={Information geometry and its applications},
  author={Amari, Shun-ichi},
  volume={194},
  year={2016},
  publisher={Springer}
}

@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-ichi},
  journal={Neural computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998}
}

@article{pascanu2013revisiting,
  title={Revisiting natural gradient for deep networks},
  author={Pascanu, Razvan and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1301.3584},
  year={2013}
}

@article{karakida2019pathological,
  title={Pathological spectra of the Fisher information metric and its variants in deep neural networks},
  author={Karakida, Ryo and Akaho, Shun-ichi and Amari, Shun-ichi},
  journal={Neural Networks},
  volume={118},
  pages={166--174},
  year={2019}
}

@article{karakida2018universal,
  title={Universal statistics of Fisher information in deep neural networks: Mean field approach},
  author={Karakida, Ryo and Akaho, Shun-ichi and Amari, Shun-ichi},
  journal={Artificial Intelligence and Statistics (AISTATS)},
  pages={1032--1041},
  year={2019}
}

% Effective Dimension and Complexity Measures
@article{maddox2020rethinking,
  title={Rethinking parameter counting in deep models: Effective dimensionality revisited},
  author={Maddox, Wesley J and Benton, Gregory and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2003.02139},
  year={2020}
}

@article{liang2019fisher,
  title={Fisher-Rao metric, geometry, and complexity of neural networks},
  author={Liang, Tong and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
  journal={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={888--896},
  year={2019}
}

@article{valle2018deep,
  title={A deep learning theory for neural networks grounded in physics},
  author={Valle-P{\'e}rez, Guillermo and Camargo, Camilo Q and Louis, Ard A},
  journal={arXiv preprint arXiv:1803.08823},
  year={2018}
}

% Double Descent and Modern Deep Learning Theory
@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019}
}

@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{advani2020high,
  title={High-dimensional dynamics of generalization error in neural networks},
  author={Advani, Madhu S and Saxe, Andrew M and Sompolinsky, Haim},
  journal={Neural Networks},
  volume={132},
  pages={428--446},
  year={2020}
}

% PAC-Bayes and Generalization Bounds
@article{mcallester1999pac,
  title={PAC-Bayesian model averaging},
  author={McAllester, David A},
  booktitle={Proceedings of the twelfth annual conference on Computational learning theory},
  pages={164--170},
  year={1999}
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={1499--1508},
  year={2019}
}

@article{neyshabur2017pac,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={30},
  year={2017}
}

% Lottery Ticket Hypothesis and Neural Architecture
@article{frankle2019lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={International Conference on Learning Representations (ICLR)},
  year={2019}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William},
  journal={International Conference on Learning Representations (ICLR)},
  year={2016}
}

% Neural Collapse and Geometry
@article{papyan2020prevalence,
  title={Prevalence of neural collapse during the terminal phase of deep learning training},
  author={Papyan, Vardan and Han, XY and Donoho, David L},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={40},
  pages={24652--24663},
  year={2020}
}

@article{fang2021mathematical,
  title={Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training},
  author={Fang, Cong and He, Hang and Long, Qi and Su, Weijie J},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={43},
  year={2021}
}

% Dimension Theory and Geometry
@book{connes1994noncommutative,
  title={Noncommutative geometry},
  author={Connes, Alain},
  year={1994},
  publisher={Academic Press}
}

@book{mandelbrot1982fractal,
  title={The fractal geometry of nature},
  author={Mandelbrot, Benoit B},
  year={1982},
  publisher={WH Freeman}
}

@article{jonsson1995function,
  title={Function spaces on subsets of R n},
  author={Jonsson, Alf and Wallin, Hans},
  journal={Mathematical Reports},
  volume={2},
  number={1},
  pages={xiv+221},
  year={1995}
}

% Classical Deep Learning
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998}
}

@article{krizhevsky2012imagenet,
  title={ImageNet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={25},
  pages={1097--1105},
  year={2012}
}

@article{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={770--778},
  year={2016}
}

% Flatness and Generalization
@article{keskar2017large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={International Conference on Learning Representations (ICLR)},
  year={2017}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997}
}

@article{jiang2020fantastic,
  title={Fantastic generalization measures and where to find them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  journal={International Conference on Learning Representations (ICLR)},
  year={2020}
}
