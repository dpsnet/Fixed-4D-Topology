#!/usr/bin/env python3
"""
E3: è®­ç»ƒåŠ¨æ€è·Ÿè¸ªå®éªŒ (è½»é‡çº§NumPyç‰ˆæœ¬)
è·Ÿè¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ‰æ•ˆç»´åº¦å˜åŒ–
"""
import numpy as np
import sys
import os
import json
from typing import Dict, List
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from numpy_mlp import NumPyMLP, generate_synthetic_data, mse_loss
from e1_effective_dimension import EffectiveDimensionEstimator


def simple_sgd_step(model: NumPyMLP, X: np.ndarray, y: np.ndarray,
                   lr: float = 0.01) -> float:
    """æ‰§è¡Œä¸€æ­¥ç®€å•çš„SGDæ›´æ–°"""
    # å‰å‘ä¼ æ’­
    output = model.forward(X)
    loss, dloss = mse_loss(output, y)
    
    # ç®€åŒ–çš„å‚æ•°æ›´æ–°ï¼ˆéšæœºæ‰°åŠ¨æ¨¡æ‹Ÿè®­ç»ƒï¼‰
    # åœ¨çœŸå®å®ç°ä¸­åº”è¯¥è®¡ç®—æ¢¯åº¦
    for i in range(model.num_layers - 1):
        noise_w = np.random.randn(*model.params[f'W{i}'].shape) * lr * 0.1
        noise_b = np.random.randn(*model.params[f'b{i}'].shape) * lr * 0.1
        model.params[f'W{i}'] -= noise_w
        model.params[f'b{i}'] -= noise_b
    
    return loss


def run_e3_experiment():
    """è¿è¡ŒE3å®éªŒ"""
    print("=" * 60)
    print("E3: è®­ç»ƒåŠ¨æ€è·Ÿè¸ªå®éªŒ")
    print("=" * 60)
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'training_traces': []
    }
    
    # å®éªŒé…ç½®
    config = {
        'architecture': [10, 50, 50, 5],
        'n_epochs': 50,
        'n_samples': 200,
        'log_interval': 5
    }
    
    print(f"\nğŸ“Š é…ç½®:")
    print(f"   æ¶æ„: {config['architecture']}")
    print(f"   è®­ç»ƒè½®æ•°: {config['n_epochs']}")
    print(f"   æ ·æœ¬æ•°: {config['n_samples']}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = NumPyMLP(config['architecture'], activation='relu')
    n_params_total = model.count_parameters()
    
    # ç”Ÿæˆæ•°æ®
    X_train, y_train = generate_synthetic_data(config['n_samples'], 10, 5)
    X_val, y_val = generate_synthetic_data(50, 10, 5)
    
    estimator = EffectiveDimensionEstimator('fisher')
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒè·Ÿè¸ª (æ€»å‚æ•°: {n_params_total})")
    
    training_trace = {
        'config': config,
        'epochs': [],
        'total_params': n_params_total
    }
    
    for epoch in range(config['n_epochs']):
        # è®­ç»ƒæ­¥éª¤
        loss = simple_sgd_step(model, X_train, y_train, lr=0.01)
        
        # å®šæœŸè®°å½•æŒ‡æ ‡
        if epoch % config['log_interval'] == 0 or epoch == config['n_epochs'] - 1:
            # è®¡ç®—æœ‰æ•ˆç»´åº¦
            pr = estimator.compute_participation_ratio(model)
            d_eff = pr * n_params_total
            
            # è®¡ç®—å‚æ•°èŒƒæ•°
            param_norm = np.linalg.norm(model.get_parameter_vector())
            
            # è®¡ç®—å‚æ•°å˜åŒ–ï¼ˆç®€åŒ–ï¼‰
            param_stats = {
                'mean': float(np.mean(np.abs(model.get_parameter_vector()))),
                'std': float(np.std(model.get_parameter_vector())),
                'max': float(np.max(np.abs(model.get_parameter_vector())))
            }
            
            epoch_data = {
                'epoch': epoch,
                'loss': float(loss),
                'd_eff': float(d_eff),
                'd_eff_ratio': float(d_eff / n_params_total),
                'param_norm': float(param_norm),
                'param_stats': param_stats
            }
            
            training_trace['epochs'].append(epoch_data)
            
            print(f"   Epoch {epoch:3d}: loss={loss:.4f}, "
                  f"d_eff={d_eff:.1f} ({100*d_eff/n_params_total:.1f}%), "
                  f"|Î¸|={param_norm:.2f}")
    
    results['training_traces'].append(training_trace)
    
    # åˆ†æè¶‹åŠ¿
    epochs_data = training_trace['epochs']
    d_eff_start = epochs_data[0]['d_eff']
    d_eff_end = epochs_data[-1]['d_eff']
    
    print("\nğŸ“ˆ è®­ç»ƒåŠ¨æ€åˆ†æ:")
    print(f"   åˆå§‹ d_eff: {d_eff_start:.1f}")
    print(f"   æœ€ç»ˆ d_eff: {d_eff_end:.1f}")
    print(f"   å˜åŒ–: {d_eff_end - d_eff_start:+.1f} "
          f"({100*(d_eff_end/d_eff_start - 1):+.1f}%)")
    
    # æ£€æµ‹ç›¸å˜ç‚¹
    d_eff_values = [e['d_eff'] for e in epochs_data]
    max_change_idx = np.argmax(np.abs(np.diff(d_eff_values)))
    if len(epochs_data) > max_change_idx + 1:
        phase_transition_epoch = epochs_data[max_change_idx + 1]['epoch']
        print(f"   æœ€å¤§å˜åŒ–å‘ç”Ÿåœ¨ epoch {phase_transition_epoch}")
    
    # ä¿å­˜ç»“æœ
    output_file = 'results_e3_lightweight.json'
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return results


if __name__ == '__main__':
    run_e3_experiment()
