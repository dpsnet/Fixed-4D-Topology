#!/usr/bin/env python3
"""
E1: æœ‰æ•ˆç»´åº¦ä¼°è®¡å™¨éªŒè¯å®éªŒ (è½»é‡çº§NumPyç‰ˆæœ¬)
éªŒè¯ä¸åŒä¼°è®¡å™¨çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§
"""
import numpy as np
import sys
import os
import json
from typing import Dict, List, Tuple
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from numpy_mlp import NumPyMLP, generate_synthetic_data, mse_loss


class EffectiveDimensionEstimator:
    """åŸºäºéšæœºåŒ–æ–¹æ³•çš„æœ‰æ•ˆç»´åº¦ä¼°è®¡å™¨"""
    
    def __init__(self, method: str = 'fisher'):
        self.method = method
        self.history = []
        
    def estimate_trace(self, model: NumPyMLP, X: np.ndarray, y: np.ndarray,
                      n_samples: int = 100) -> float:
        """ä½¿ç”¨éšæœºåŒ–ä¼°è®¡è¿¹"""
        n_params = model.count_parameters()
        
        if self.method == 'fisher':
            # åŸºäºFisherä¿¡æ¯çŸ©é˜µçš„è¿¹ä¼°è®¡
            traces = []
            for _ in range(min(n_samples, 50)):  # é™åˆ¶æ ·æœ¬æ•°
                z = np.random.randn(X.shape[0], X.shape[1])
                fisher = model.compute_fisher(z, y, mse_loss)
                traces.append(np.trace(fisher))
            return np.mean(traces)
        
        elif self.method == 'random':
            # éšæœºæŠ•å½±ä¼°è®¡
            dim = min(n_params, 1000)
            A = np.random.randn(dim, n_params)
            ATA = A @ A.T
            return np.trace(ATA)
        
        else:
            return float(n_params)
    
    def estimate_rank(self, model: NumPyMLP, X: np.ndarray, 
                     threshold: float = 1e-3) -> int:
        """ä¼°è®¡æœ‰æ•ˆç§©"""
        # ç®€åŒ–çš„ç§©ä¼°è®¡ï¼šåŸºäºå‚æ•°åˆ†å¸ƒ
        params = model.get_parameter_vector()
        
        # è®¡ç®—å¥‡å¼‚å€¼ï¼ˆè¿‘ä¼¼ï¼‰
        n = min(len(params), 1000)
        sample = np.random.choice(params, n, replace=False)
        
        # ä½¿ç”¨æ–¹å·®ä½œä¸ºæœ‰æ•ˆç»´åº¦çš„ä»£ç†
        variance = np.var(sample)
        effective = int(np.sum(np.abs(sample) > threshold * np.std(params)))
        
        return min(effective, model.count_parameters())
    
    def compute_participation_ratio(self, model: NumPyMLP) -> float:
        """è®¡ç®—å‚ä¸ç‡ï¼ˆParticipation Ratioï¼‰"""
        params = model.get_parameter_vector()
        squared = params ** 2
        sum_sq = np.sum(squared)
        sum_fourth = np.sum(squared ** 2)
        
        if sum_fourth > 0:
            return (sum_sq ** 2) / (len(params) * sum_fourth)
        return 1.0


def run_e1_experiment():
    """è¿è¡ŒE1å®éªŒ"""
    print("=" * 60)
    print("E1: æœ‰æ•ˆç»´åº¦ä¼°è®¡å™¨éªŒè¯å®éªŒ")
    print("=" * 60)
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'configurations': [],
        'summary': {}
    }
    
    # å®éªŒé…ç½®
    configs = [
        {'name': 'Small Network', 'layers': [10, 20, 5], 'n_samples': 100},
        {'name': 'Medium Network', 'layers': [20, 40, 40, 10], 'n_samples': 200},
        {'name': 'Wide Shallow', 'layers': [10, 100, 5], 'n_samples': 100},
        {'name': 'Deep Narrow', 'layers': [10, 15, 15, 15, 5], 'n_samples': 100},
    ]
    
    estimators = ['fisher', 'random']
    
    for cfg in configs:
        print(f"\nğŸ“Š Testing: {cfg['name']}")
        print(f"   Architecture: {cfg['layers']}")
        
        # åˆ›å»ºæ¨¡å‹
        model = NumPyMLP(cfg['layers'], activation='relu')
        n_params = model.count_parameters()
        
        # ç”Ÿæˆæ•°æ®
        X, y = generate_synthetic_data(
            cfg['n_samples'], cfg['layers'][0], cfg['layers'][-1]
        )
        
        config_result = {
            'name': cfg['name'],
            'architecture': cfg['layers'],
            'total_params': n_params,
            'estimates': {}
        }
        
        # è¿è¡Œä¸åŒä¼°è®¡å™¨
        for est_name in estimators:
            estimator = EffectiveDimensionEstimator(method=est_name)
            
            trace_est = estimator.estimate_trace(model, X, y)
            rank_est = estimator.estimate_rank(model, X)
            pr_est = estimator.compute_participation_ratio(model)
            
            # è®¡ç®—æœ‰æ•ˆç»´åº¦
            d_eff_trace = min(trace_est / n_params * n_params, n_params)
            d_eff_rank = rank_est
            d_eff_pr = pr_est * n_params
            
            config_result['estimates'][est_name] = {
                'trace_estimate': float(trace_est),
                'rank_estimate': int(rank_est),
                'participation_ratio': float(pr_est),
                'd_eff_trace': float(d_eff_trace),
                'd_eff_rank': int(d_eff_rank),
                'd_eff_pr': float(d_eff_pr)
            }
            
            print(f"   Estimator: {est_name}")
            print(f"     Total params: {n_params}")
            print(f"     Trace d_eff: {d_eff_trace:.1f}")
            print(f"     Rank d_eff: {d_eff_rank}")
            print(f"     PR d_eff: {d_eff_pr:.1f}")
        
        # è®¡ç®—ä¸€è‡´æ€§
        d_eff_values = [
            config_result['estimates']['fisher']['d_eff_pr'],
            config_result['estimates']['random']['d_eff_pr']
        ]
        consistency = 1 - np.std(d_eff_values) / (np.mean(d_eff_values) + 1e-10)
        config_result['consistency'] = float(consistency)
        
        print(f"   Consistency: {consistency:.3f}")
        
        results['configurations'].append(config_result)
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    all_consistencies = [c['consistency'] for c in results['configurations']]
    results['summary'] = {
        'mean_consistency': float(np.mean(all_consistencies)),
        'min_consistency': float(np.min(all_consistencies)),
        'max_consistency': float(np.max(all_consistencies))
    }
    
    print("\n" + "=" * 60)
    print("E1 å®éªŒæ€»ç»“")
    print("=" * 60)
    print(f"å¹³å‡ä¸€è‡´æ€§: {results['summary']['mean_consistency']:.3f}")
    print(f"ä¸€è‡´æ€§èŒƒå›´: [{results['summary']['min_consistency']:.3f}, "
          f"{results['summary']['max_consistency']:.3f}]")
    
    # ä¿å­˜ç»“æœ
    output_file = 'results_e1_lightweight.json'
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return results


if __name__ == '__main__':
    run_e1_experiment()
