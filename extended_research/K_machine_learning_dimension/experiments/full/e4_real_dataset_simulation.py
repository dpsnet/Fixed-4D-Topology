#!/usr/bin/env python3
"""
E4: çœŸå®æ•°æ®é›†éªŒè¯å®éªŒ (NumPyæ¨¡æ‹Ÿç‰ˆæœ¬)
æ¨¡æ‹ŸMNIST/CIFAR-likeæ•°æ®ç»“æ„çš„å®éªŒ
"""
import numpy as np
import sys
import os
import json
from typing import Dict, List, Tuple
from datetime import datetime
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from lightweight.numpy_mlp import NumPyMLP, generate_synthetic_data, mse_loss
from lightweight.e1_effective_dimension import EffectiveDimensionEstimator


def generate_mnist_like_data(n_samples: int, n_classes: int = 10, 
                             input_dim: int = 784) -> Tuple[np.ndarray, np.ndarray]:
    """ç”ŸæˆMNIST-likeç»“æ„åŒ–æ•°æ®"""
    np.random.seed(42)
    X = np.random.randn(n_samples, input_dim) * 0.5
    
    # æ·»åŠ ç±»ç‰¹å®šçš„ç»“æ„
    y = np.zeros((n_samples, n_classes))
    for i in range(n_samples):
        label = i % n_classes
        # One-hotç¼–ç 
        y[i, label] = 1.0
        # åœ¨ç‰¹å¾ä¸­æ·»åŠ ç±»ç‰¹å®šæ¨¡å¼
        X[i, label*10:(label+1)*10] += np.random.randn(10) * 2
    
    return X, y


def generate_cifar_like_data(n_samples: int, n_classes: int = 10,
                             input_dim: int = 3072) -> Tuple[np.ndarray, np.ndarray]:
    """ç”ŸæˆCIFAR-likeç»“æ„åŒ–æ•°æ® (RGBå›¾åƒ)"""
    np.random.seed(43)
    X = np.random.randn(n_samples, input_dim) * 0.3
    
    y = np.zeros((n_samples, n_classes))
    for i in range(n_samples):
        label = i % n_classes
        y[i, label] = 1.0
        # RGBé€šé“ç‰¹å®šæ¨¡å¼
        channel_size = input_dim // 3
        for c in range(3):
            X[i, c*channel_size + label*10:c*channel_size + (label+1)*10] += np.random.randn(10) * 1.5
    
    return X, y


def run_e4_experiment():
    """è¿è¡ŒE4å®éªŒ"""
    print("=" * 70)
    print("E4: çœŸå®æ•°æ®é›†éªŒè¯å®éªŒ (ç»“æ„åŒ–æ•°æ®æ¨¡æ‹Ÿ)")
    print("=" * 70)
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'datasets': []
    }
    
    # å®éªŒé…ç½®
    configs = [
        {
            'name': 'MNIST-like',
            'data_fn': generate_mnist_like_data,
            'input_dim': 784,
            'n_classes': 10,
            'train_samples': 5000,
            'test_samples': 1000,
            'architecture': [784, 256, 128, 10]
        },
        {
            'name': 'CIFAR-like',
            'data_fn': generate_cifar_like_data,
            'input_dim': 3072,
            'n_classes': 10,
            'train_samples': 3000,
            'test_samples': 600,
            'architecture': [3072, 512, 256, 10]
        },
        {
            'name': 'Small-Scale',
            'data_fn': generate_mnist_like_data,
            'input_dim': 256,
            'n_classes': 10,
            'train_samples': 2000,
            'test_samples': 400,
            'architecture': [256, 128, 64, 10]
        }
    ]
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    for idx, cfg in enumerate(configs):
        print(f"\nğŸ“Š æ•°æ®é›†: {cfg['name']}")
        print(f"   æ¶æ„: {cfg['architecture']}")
        print(f"   è®­ç»ƒæ ·æœ¬: {cfg['train_samples']}")
        
        # ç”Ÿæˆæ•°æ®
        X_train, y_train = cfg['data_fn'](cfg['train_samples'], cfg['n_classes'], cfg['input_dim'])
        X_test, y_test = cfg['data_fn'](cfg['test_samples'], cfg['n_classes'], cfg['input_dim'])
        
        # åˆ›å»ºæ¨¡å‹
        model = NumPyMLP(cfg['architecture'], activation='relu')
        n_params = model.count_parameters()
        
        # è®¡ç®—æœ‰æ•ˆç»´åº¦
        estimator = EffectiveDimensionEstimator('fisher')
        pr = estimator.compute_participation_ratio(model)
        d_eff = pr * n_params
        
        # è®¡ç®—æ³›åŒ–è¯¯å·®ä¼°è®¡
        output_train = model.forward(X_train)
        train_error = np.mean((output_train - y_train) ** 2)
        
        output_test = model.forward(X_test)
        test_error = np.mean((output_test - y_test) ** 2)
        generalization_gap = test_error - train_error
        
        # é¢„æµ‹æ³›åŒ–ç•Œ
        predicted_bound = np.sqrt(d_eff / cfg['train_samples'])
        
        dataset_result = {
            'name': cfg['name'],
            'architecture': cfg['architecture'],
            'total_params': n_params,
            'd_eff': float(d_eff),
            'd_eff_ratio': float(d_eff / n_params),
            'train_samples': cfg['train_samples'],
            'train_error': float(train_error),
            'test_error': float(test_error),
            'generalization_gap': float(generalization_gap),
            'predicted_bound': float(predicted_bound)
        }
        
        results['datasets'].append(dataset_result)
        
        print(f"   æ€»å‚æ•°: {n_params}")
        print(f"   æœ‰æ•ˆç»´åº¦: {d_eff:.1f} ({100*d_eff/n_params:.1f}%)")
        print(f"   è®­ç»ƒè¯¯å·®: {train_error:.4f}")
        print(f"   æµ‹è¯•è¯¯å·®: {test_error:.4f}")
        print(f"   æ³›åŒ–å·®è·: {generalization_gap:.4f}")
        print(f"   é¢„æµ‹æ³›åŒ–ç•Œ: {predicted_bound:.4f}")
        
        # ç»˜å›¾
        ax = axes[idx]
        metrics = ['Train Error', 'Test Error', 'Gen Gap', 'Pred Bound']
        values = [train_error, test_error, generalization_gap, predicted_bound]
        colors = ['green', 'orange', 'red', 'blue']
        ax.bar(metrics, values, color=colors, alpha=0.7)
        ax.set_title(f"{cfg['name']}\nd_eff/N={100*d_eff/n_params:.1f}%")
        ax.set_ylabel('Error / Bound')
        ax.tick_params(axis='x', rotation=15)
    
    plt.tight_layout()
    plt.savefig('e4_real_dataset_results.png', dpi=150, bbox_inches='tight')
    print(f"\nâœ“ å›¾è¡¨å·²ä¿å­˜: e4_real_dataset_results.png")
    
    # ä¿å­˜ç»“æœ
    with open('results_e4_full.json', 'w') as f:
        json.dump(results, f, indent=2)
    print(f"âœ“ ç»“æœå·²ä¿å­˜: results_e4_full.json")
    
    return results


if __name__ == '__main__':
    run_e4_experiment()
