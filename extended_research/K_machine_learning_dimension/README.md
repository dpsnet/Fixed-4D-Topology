# Kæ–¹å‘: æœºå™¨å­¦ä¹ ç»´åº¦
## K Direction: Machine Learning Dimension

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.10+-red.svg)](https://pytorch.org/)

**åŸºäºFisherä¿¡æ¯çš„ç¥ç»ç½‘ç»œæœ‰æ•ˆç»´åº¦ç†è®ºæ¡†æ¶**

---

## ğŸ¯ æ ¸å¿ƒæ€æƒ³

ç¥ç»ç½‘ç»œçš„æœ‰æ•ˆç»´åº¦(effective dimension) $d_{\text{eff}}$ è¡¡é‡æ¨¡å‹çš„"çœŸå®"å¤æ‚åº¦ï¼Œè¿œå°äºå‚æ•°æ•°é‡ $D$ã€‚æœ¬æ¡†æ¶æä¾›ï¼š

- **ä¸¥æ ¼æ•°å­¦å®šä¹‰**: åŸºäºFisherä¿¡æ¯çŸ©é˜µ
- **åŠ¨æ€æ¼”åŒ–æ–¹ç¨‹**: æè¿°è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç»´åº¦å˜åŒ–
- **æ³›åŒ–è¯¯å·®ç•Œ**: $O(\sqrt{d_{\text{eff}}/n})$ æ ·æœ¬å¤æ‚åº¦
- **Dimensionicsç»Ÿä¸€**: ä¸ç‰©ç†ç»´åº¦ç†è®ºçš„è·¨å­¦ç§‘è¿æ¥

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
K_machine_learning_dimension/
â”œâ”€â”€ theory/                          # ç†è®ºæ–‡æ¡£
â”‚   â”œâ”€â”€ K1.1_Fisher_Information.md   # Fisherä¿¡æ¯åŸºç¡€
â”‚   â”œâ”€â”€ K1.2_Effective_Dimension.md  # æœ‰æ•ˆç»´åº¦å®šä¹‰
â”‚   â”œâ”€â”€ K1.3_Training_Dynamics.md    # è®­ç»ƒåŠ¨æ€æ–¹ç¨‹
â”‚   â”œâ”€â”€ K1.4_Generalization_Bounds.md # æ³›åŒ–ç•Œè¯æ˜
â”‚   â”œâ”€â”€ K1.5_Dimensionics_Connection.md # Dimensionicsè¿æ¥
â”‚   â””â”€â”€ K_DIRECTION_PAPER.md         # æ•´åˆè®ºæ–‡æ¡†æ¶
â”‚
â”œâ”€â”€ code/                            # Pythonå·¥å…·åŒ…
â”‚   â”œâ”€â”€ neural_dimension/            # ä¸»åŒ…
â”‚   â”‚   â”œâ”€â”€ core/                    # æ ¸å¿ƒæ¨¡å—
â”‚   â”‚   â”‚   â”œâ”€â”€ fisher_information.py
â”‚   â”‚   â”‚   â”œâ”€â”€ effective_dimension.py
â”‚   â”‚   â”‚   â””â”€â”€ dimension_dynamics.py
â”‚   â”‚   â”œâ”€â”€ models/                  # æ¨¡å‹æ¶æ„
â”‚   â”‚   â”‚   â”œâ”€â”€ standard_architectures.py
â”‚   â”‚   â”‚   â””â”€â”€ lottery_ticket.py
â”‚   â”‚   â”œâ”€â”€ visualization/           # å¯è§†åŒ–
â”‚   â”‚   â”‚   â””â”€â”€ dimension_plots.py
â”‚   â”‚   â””â”€â”€ experiments/             # å®éªŒå®ç°
â”‚   â”‚       â”œâ”€â”€ double_descent.py
â”‚   â”‚       â””â”€â”€ neural_collapse.py
â”‚   â””â”€â”€ setup.py                     # å®‰è£…é…ç½®
â”‚
â”œâ”€â”€ experiments/                     # å®éªŒè„šæœ¬
â”‚   â”œâ”€â”€ protocols/                   # å®éªŒåè®®
â”‚   â”‚   â””â”€â”€ EXPERIMENTS_PROTOCOL.md
â”‚   â””â”€â”€ scripts/                     # å¯è¿è¡Œè„šæœ¬
â”‚       â”œâ”€â”€ E1_effective_dim_baseline.py
â”‚       â”œâ”€â”€ E2_training_dynamics.py
â”‚       â”œâ”€â”€ E3_double_descent.py
â”‚       â”œâ”€â”€ E4_neural_collapse.py
â”‚       â”œâ”€â”€ E5_lottery_ticket.py
â”‚       â””â”€â”€ E6_generalization_bound.py
â”‚
â”œâ”€â”€ integration/                     # è·¨æ–¹å‘è¿æ¥
â”‚   â”œâ”€â”€ KH_QUANTUM_NN.md             # K-Hè¿æ¥
â”‚   â”œâ”€â”€ KI_NETWORK_NN.md             # K-Iè¿æ¥
â”‚   â”œâ”€â”€ KJ_RANDOM_INIT.md            # K-Jè¿æ¥
â”‚   â”œâ”€â”€ K_CROSS_DIRECTION_FRAMEWORK.md # ç»Ÿä¸€æ¡†æ¶
â”‚   â””â”€â”€ JOINT_EXPERIMENTS.md         # è”åˆå®éªŒè®¾è®¡
â”‚
â”œâ”€â”€ notebooks/                       # Jupyteræ¼”ç¤º
â”‚   â””â”€â”€ (å¾…åˆ›å»º)
â”‚
â”œâ”€â”€ tests/                           # å•å…ƒæµ‹è¯•
â”‚   â””â”€â”€ (å¾…åˆ›å»º)
â”‚
â”œâ”€â”€ PLAN.md                          # å¹¶è¡Œå¼€å‘è®¡åˆ’
â”œâ”€â”€ PROGRESS.md                      # è¿›åº¦è¿½è¸ª
â””â”€â”€ README.md                        # æœ¬æ–‡ä»¶
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
cd Fixed-4D-Topology/extended_research/K_machine_learning_dimension

# å®‰è£…åŒ…
pip install code/

# æˆ–å¼€å‘æ¨¡å¼
pip install -e code/
```

### åŸºç¡€ç”¨æ³•

```python
import torch
from neural_dimension import FisherInformationMatrix, EffectiveDimensionCalculator
from neural_dimension.models import TwoLayerMLP

# åˆ›å»ºæ¨¡å‹
model = TwoLayerMLP(hidden_dim=128)

# å‡†å¤‡æ•°æ®
train_loader = ...  # PyTorch DataLoader

# è®¡ç®—Fisherä¿¡æ¯çŸ©é˜µ
fisher_calc = FisherInformationMatrix(model, sigma=1.0)
fisher_matrix = fisher_calc.compute_diagonal_fisher(train_loader)

# è®¡ç®—æœ‰æ•ˆç»´åº¦
dim_calc = EffectiveDimensionCalculator(fisher_calc)
dimensions = dim_calc.compute_all_dimensions(n_samples=1000)

print(f"æœ‰æ•ˆç»´åº¦: {dimensions['fisher_effective_dimension']:.2f}")
print(f"æ€»å‚æ•°: {dimensions['total_parameters']}")
print(f"ç»´åº¦å‹ç¼©æ¯”: {dimensions['reduction_ratio']:.4f}")
```

### è¿è¡Œå®éªŒ

```bash
# E1: æœ‰æ•ˆç»´åº¦åŸºå‡†æµ‹é‡
python experiments/scripts/E1_effective_dim_baseline.py

# E2: è®­ç»ƒåŠ¨æ€è¿½è¸ª
python experiments/scripts/E2_training_dynamics.py

# E3: åŒä¸‹é™éªŒè¯
python experiments/scripts/E3_double_descent.py

# E4: ç¥ç»å´©å¡Œåˆ†æ
python experiments/scripts/E4_neural_collapse.py

# E5: å½©ç¥¨ç¥¨å‡è®¾
python experiments/scripts/E5_lottery_ticket.py

# E6: æ³›åŒ–ç•ŒéªŒè¯
python experiments/scripts/E6_generalization_bound.py
```

---

## ğŸ“Š æ ¸å¿ƒç†è®º

### æœ‰æ•ˆç»´åº¦å®šä¹‰

åŸºäºFisherä¿¡æ¯çŸ©é˜µ $F$:

$$d_{\text{eff}} = \frac{(\text{tr} F)^2}{\text{tr}(F^2)} = \frac{(\sum_i \lambda_i)^2}{\sum_i \lambda_i^2}$$

### å…³é”®æ€§è´¨

- **èŒƒå›´**: $1 \leq d_{\text{eff}} \leq D$ (æ€»å‚æ•°)
- **å°ºåº¦ä¸å˜æ€§**: å¯¹ $F$ çš„ç¼©æ”¾ä¸å˜
- **å•è°ƒæ€§**: $F_1 \preceq F_2 \Rightarrow d_{\text{eff}}^{(1)} \leq d_{\text{eff}}^{(2)}$

### æ³›åŒ–ç•Œ

ä»¥é«˜æ¦‚ç‡:

$$R \leq \hat{R} + \mathcal{O}\left(\sqrt{\frac{d_{\text{eff}} \ln(n/d_{\text{eff}})}{n}}\right)$$

### ç»´åº¦æ¼”åŒ–æ–¹ç¨‹

$$\frac{\partial d_{\text{eff}}}{\partial t} = \alpha \mathcal{L}(d_{\text{data}} - d_{\text{eff}}) - \beta d_{\text{eff}} R$$

---

## ğŸ”¬ å®éªŒæ¦‚è§ˆ

| å®éªŒ | ç›®æ ‡ | å…³é”®ç»“æœ |
|------|------|----------|
| **E1** | åŸºå‡†æµ‹é‡ | ä¸åŒæ¶æ„çš„ $d_{\text{eff}}$ æ¯”è¾ƒ |
| **E2** | è®­ç»ƒåŠ¨æ€ | $d_{\text{eff}}(t)$ æ¼”åŒ–æ›²çº¿ |
| **E3** | åŒä¸‹é™ | ç»´åº¦è§£é‡Šçš„åŒä¸‹é™éªŒè¯ |
| **E4** | ç¥ç»å´©å¡Œ | NC1/NC2/NC3 ä¸ç»´åº¦å…³ç³» |
| **E5** | å½©ç¥¨ç¥¨ | è·èƒœç¥¨åˆ¸çš„ç»´åº¦ç‰¹æ€§ |
| **E6** | æ³›åŒ–ç•Œ | ç†è®ºç•Œçš„ç»éªŒéªŒè¯ |

---

## ğŸ”— è·¨æ–¹å‘è¿æ¥

Kæ–¹å‘ä¸ä»¥ä¸‹æ–¹å‘å»ºç«‹è¿æ¥:

- **Hæ–¹å‘ (é‡å­ç»´åº¦)**: é‡å­ç¥ç»ç½‘ç»œçš„æœ‰æ•ˆç»´åº¦
- **Iæ–¹å‘ (ç½‘ç»œå‡ ä½•)**: ç¥ç»ç½‘ç»œä½œä¸ºå¤æ‚ç½‘ç»œ
- **Jæ–¹å‘ (éšæœºåˆ†å½¢)**: æ¸—æµç†è®ºä¸åˆå§‹åŒ–

è¯¦è§ `integration/` ç›®å½•ã€‚

---

## ğŸ“– æ–‡æ¡£

- **ç†è®º**: è§ `theory/` ç›®å½•
- **å®éªŒåè®®**: è§ `experiments/protocols/`
- **APIæ–‡æ¡£**: (å¾…ç”Ÿæˆ)

---

## ğŸ™ è‡´è°¢

### ç ”ç©¶èµ·æº

Kæ–¹å‘ï¼ˆæœºå™¨å­¦ä¹ ç»´åº¦ï¼‰æ˜¯ **Fixed-4D-Topologyç»Ÿä¸€æ¡†æ¶** çš„æ‰©å±•ç ”ç©¶ï¼Œ
ç”±åŒä¸€ç ”ç©¶è€…å‘èµ·ã€‚è¯¥æ–¹å‘å°†ç»´åº¦ç†è®ºä»ç‰©ç†-æ•°å­¦é¢†åŸŸæ‰©å±•åˆ°æœºå™¨å­¦ä¹ é¢†åŸŸï¼Œ
å½¢æˆK-H-I-Jè·¨æ–¹å‘ç ”ç©¶ä½“ç³»çš„ä¸€éƒ¨åˆ†ã€‚

**ç ”ç©¶æ¼”è¿›** (æ ¹æ®Gitæäº¤å†å²):
- **2026-01-27**: åŸºç¡€æ¡†æ¶å»ºç«‹ (Fundamental-Mathematics, Physical-Applicationsç­‰)
- **2026-02-03**: æ‰©å±•æ¡†æ¶å»ºç«‹ (Advanced-Physics-Framework, Computational-Frameworkç­‰)
- **2026-02-07**: Fixed-4D-Topology v1.0.0å‘å¸ƒï¼ŒA~Gæ–¹å‘æ·±åº¦ç ”ç©¶å¯åŠ¨
- **2026-02-07**: H-Jæ‰©å±•ç ”ç©¶å¯åŠ¨ (é‡å­ç»´åº¦ã€ç½‘ç»œå‡ ä½•ã€éšæœºåˆ†å½¢)
- **2026-02-09**: **Kæ–¹å‘å¯åŠ¨**: æœºå™¨å­¦ä¹ ç»´åº¦ç†è®º

### æ–¹æ³•è®º

- **äººç±»ç ”ç©¶å‘˜**: ç ”ç©¶æ„¿æ™¯ã€æ¦‚å¿µæŒ‡å¯¼ã€è´¨é‡æŠŠæ§
- **Kimi 2.5 Agent**: æ•°å­¦æ¨å¯¼ã€ä»£ç å®ç°ã€æ–‡æ¡£ç¼–å†™
- é‡‡ç”¨äººæœºåä½œèŒƒå¼ï¼Œè¯šå®æŠ«éœ²AIè´¡çŒ®

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹ä¸»ä»“åº“çš„ CONTRIBUTING.mdã€‚

---

## ğŸ“„ å¼•ç”¨

```bibtex
@article{k_direction_2026,
  title={Neural Network Effective Dimension: A Dimensionics Framework},
  author={Human Researcher and Kimi 2.5 Agent},
  year={2026},
  url={https://github.com/dpsnet/Fixed-4D-Topology}
}
```

---

## ğŸ“ è®¸å¯è¯

MIT License - è§ä¸»ä»“åº“ LICENSE æ–‡ä»¶ã€‚

---

**ç ”ç©¶æ–¹æ³•è®º**: æœ¬ç ”ç©¶é‡‡ç”¨äººæœºåä½œèŒƒå¼ã€‚Kimi 2.5 Agent ç”Ÿæˆæ‰€æœ‰å†…å®¹ï¼Œäººç±»æä¾›æ–¹å‘æŒ‡å¯¼ã€‚

**çŠ¶æ€**: å¼€å‘é˜¶æ®µåŸºæœ¬å®Œæˆï¼Œè¿›å…¥å®éªŒéªŒè¯é˜¶æ®µã€‚

**æœ€åæ›´æ–°**: 2026-02-09
